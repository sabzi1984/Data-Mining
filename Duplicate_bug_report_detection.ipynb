{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "TP1_Fouille de donnee.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vapGWIBgFt0q"
      },
      "source": [
        "# INF8111 - Fouille de données\n",
        "\n",
        "## TP1 Automne 2020 - Duplicate Bug Report Detection\n",
        "\n",
        "##### Membres de l'équipe / Team members:\n",
        "\n",
        "    - Sabzi Dizajyekan (2078921) 1\n",
        "    - Desclaux  (2097696) 2\n",
        "    - Berrais-Sanchez  (2092882) 3\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgkUX6Ki0W9s"
      },
      "source": [
        "## 1 - Résumé / Overview\n",
        "\n",
        "À cause de la complexité des systèmes informatiques, les bogues logiciels sont courants. Les entreprises, et en particulier les plus grosses, utilisent un *Bug Tracking System* (BTS), aussi appelé *Issue Tracking System*, pour organiser et suivre les rapports de bogues. En plus des développeurs et des testeurs, de nombreux projets (notamment ceux libres de droits) permettent aux utilisateurs de soumettre un rapport de bogues dans leur BTS. Pour ce faire, les utilisateurs doivent remplir un formulaire avec plusieurs champs. La majorité de ces champs contient des données catégoriques et accepte uniquement des valeurs prédéfinies (composant, version du produit et du système, etc.). Deux autres champs importants sont le résumé ou *summary* en anglais, et la description. Les utilisateurs sont libres d’écrire ce qu’ils veulent dans ces deux champs avec pour seule contrainte le nombre de caractères. La soumission de ces champs crée une page que l’on appelle rapport de bogue et qui contient toute l’information à propos du bogue.\n",
        "\n",
        "Par manque de communication et de synchronisation, les utilisateurs ne savent pas toujours qu’un bogue a déjà été soumis et peuvent donc le soumettre à nouveau. Identifier les rapports qui correspondent au même bogue (duplicata) est une tache importante des BTSs et est le sujet de ce TP. Notre objectif est de développer un système qui comparera les nouveaux rapports de bogues avec ceux déjà soumis en fonction de leur similarité textuelle. La liste triée des rapports les plus similaires sera utilisée par un opérateur humain pour identifier manuellement si un rapport est un duplicata.\n",
        "\n",
        "---\n",
        "\n",
        "Due to the complexity of software systems, software bugs are prevalent. Companies, especially the larger ones, usually use a Bug Tracking System (BTS), also called Issue Tracking System, to manage and track records of bugs. Besides developers and testers, many projects, mainly open source projects, allow users to report new bugs in their BTS.\n",
        "To do that, users have to fill out a form with multiple fields. An important subset of\n",
        "these fields provides categorical data and only accepts values that range from a fixed list of\n",
        "options (e.g. component, version and product of the system). Two other important fields\n",
        "are the summary and the description. The users are free to write anything in both fields\n",
        "with the only constraint that the summary has a maximum number of characters. The\n",
        "submission of a form creates a page, called bug report or issue report which contains all\n",
        "the information about a bug.\n",
        "\n",
        "Due to the lack of communication and synchronization, users may not be aware that a specific bug was already submitted and may report it again. Identifying duplicate bug reports is an important task in the BTSs and is the subject of this TP. Our objective is to develop a system that will compare a new bug report with the already submitted ones and rank them based on textual similarity. This ranked list will be used by a triager to manually identify whether a report is a duplicate or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFsBsrFzFt0r"
      },
      "source": [
        "# 2 Installation / Setup\n",
        "\n",
        "Pour ce TP, vous aurez besoin des librairies `numpy`, `sklearn` et `scipy` (que vous avez sans doute déjà), ainsi que la librairie `nltk`, qui est une libraire utilisée pour faire du traitement du language (Natural Language Processing, NLP). Installez les libraires en question et exécutez le code ci-dessous :\n",
        "\n",
        "---\n",
        "\n",
        "For this assignment, you need the `numpy`, `sklearn` and `scipy` libraries (which you may already have), as well as the `nltk` library, which is used for Natural Language Processing (NLP). Please run the code below to install the packages needed for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8r8BtgdFt0u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "c4cb6b49-0f33-4486-8e64-fd5f8f25e777"
      },
      "source": [
        "                                                                                                                                                                                                                # If you want, you can use anaconda and install after nltk library\n",
        "# pip install --user numpy\n",
        "# pip install --user sklearn\n",
        "# pip install --user scipy\n",
        "# pip install --user nltk\n",
        "# pip install --user tqdm\n",
        "\n",
        "\n",
        "#python\n",
        "#import nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.18.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGq_fsnCFt02"
      },
      "source": [
        "## 3 - Jeu de données / Data\n",
        "\n",
        "Téléchargez l'archive à l'adresse suivante: https://drive.google.com/file/d/14BrFaOiVDIcpjwpm5iAkAy0v_moZyGiP/view?usp=sharing\n",
        "\n",
        "In this zip file, there are: \n",
        "\n",
        "L'archive contient:\n",
        "1. test.json: Ce fichier contient les duplicate bug reports that will be used to evaluate our system.\n",
        "2. threads: Ce dossier contient le code HTML des bug report. Chaque fichier HTML est nommé selon le motif **bug_report_id.html**.\n",
        "\n",
        "\n",
        "L'image ci-dessous illustre un exemple de bug report:\n",
        "\n",
        "![https://ibb.co/tqyRM4L](bug_report.png)\n",
        "\n",
        "- A : identifiant du bug report\n",
        "- B : date de création\n",
        "- C : résumé\n",
        "- D : composant\n",
        "- E : produit\n",
        "- F : l'identifiant du rapport dont le bug report est dupliqué\n",
        "- G : description\n",
        "\n",
        "\n",
        "Le script suivant charge le jeu de données de test et définit certaines variables globales:\n",
        "\n",
        "---\n",
        "\n",
        "Please download the zip file at the following adress: https://drive.google.com/file/d/14BrFaOiVDIcpjwpm5iAkAy0v_moZyGiP/view?usp=sharing\n",
        "\n",
        "This archive contains: \n",
        "\n",
        "1. test.json: This file contains duplicate bug reports that will be used to evaluate our system.\n",
        "2. bug_reports: It is a folder that contains the bug report html source. Each html file name follows the pattern **bug_report_id.html**.\n",
        "\n",
        "\n",
        "Figure below depicts an bug report page example:\n",
        "\n",
        "![https://ibb.co/tqyRM4L](bug_report.png)\n",
        "\n",
        "\n",
        "- A : bug report id\n",
        "- B : creation date\n",
        "- C : summary\n",
        "- D : component\n",
        "- E : product\n",
        "- F : the report id which the bug report is duplicate\n",
        "- G : description\n",
        "\n",
        " The following script loads the test dataset and define some global variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zq49mgO08b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "727d5772-9e96-4c76-b20e-24771b9f7a85"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdRjO-NbFt04"
      },
      "source": [
        "import os\n",
        "\n",
        "# define the folder path that contain the data\n",
        "# FOLDER_PATH = \"Define folder path that contain threads folder and test.json\"\n",
        "FOLDER_PATH = \"/content/drive/My Drive/Colab Notebooks/Fouille de donnees/TP1/INF8111_2020_fall_tp1_dataset/dataset\"\n",
        "PAGE_FOLDER = os.path.join(FOLDER_PATH, 'bug_reports')\n",
        "\n",
        "\n",
        "\n",
        "# Load the evaluation dataset\n",
        "import json\n",
        "\n",
        "\n",
        "test = json.load(open(os.path.join(FOLDER_PATH,'test.json')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1WXzfKOFt08"
      },
      "source": [
        "# 4 - Web scraping\n",
        "\n",
        "\"Le *web scraping* (parfois appelé harvesting) est une technique d'extraction du contenu de sites Web, via un script ou un programme, dans le but de le transformer pour permettre son utilisation dans un autre contexte, par exemple le référencement.\" [Wikipedia](https://fr.wikipedia.org/wiki/Web_scraping)\n",
        "\n",
        "---\n",
        "\n",
        "*Web scraping* also called harvesting consists in extracting relevant data from web pages and prepare it for computational analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyWZeHkdFt08"
      },
      "source": [
        "## 4.1 - Question 1 (2 points)\n",
        "\n",
        "Implémentez *extract_data_from_page*. Cette fonction extrait l'information suivante du code HTML : identifiant du rapport de bogue, date de création, titre, produit, composant, identifiant du rapport de bogue bug dont il est un duplicata, résumé et description. \n",
        "\n",
        "\n",
        "La fonction *extract_data_from_page* retourne un dictionnaire avec la structure suivante :\n",
        "```\n",
        " {\"report_id\": int, \n",
        "  \"dup_id\": int or None (the report id which it is duplicate), \n",
        "  \"component\": string, \n",
        "  \"product\": string, \n",
        "  \"summary\": str, \n",
        "  \"description\": string, \n",
        "  \"creation_date\": int} \n",
        "```\n",
        "\n",
        "Par exemple, quand la fonction *extract_data_from_page* reçoit le rapport \"bug_report/7526.html\", elle retourne :\n",
        "\n",
        "```\n",
        "{\n",
        "\"report_id\": 7526,\n",
        "\"dup_id\": 7799,\n",
        "\"product\": \"core graveyard\",\n",
        "\"component\":  tracking,\n",
        "\"summary\": \"Apprunner crashes on exit\",\n",
        "\"description\": \"Apprunner crashes on exit, without fail. The browser window closes, but the\n",
        "console window hangs around. I end up having to kill winoldap with the \\\\\"Close\n",
        "Program\\\\\" dialog (Ctrl-Alt-Del).\",\n",
        "\"creation_date\": 928396140\n",
        "}\n",
        "```\n",
        "\n",
        "**La date de création doit être représentée comme un timestamp (entier). Si un bug n'est pas un duplicata, alors dup_id doit être None.**\n",
        "\n",
        "*Indice: lxml parse est plus rapide que html.parser*\n",
        "\n",
        "---\n",
        "\n",
        "Implement *extract_data_from_page*. This function extracts the following information from the html: bug report id, creation date, title, product, component, the report id which it is duplicate, summary and description.\n",
        "\n",
        "The *extract_data_from_page* function returns a dictionary with the following structure:\n",
        "```\n",
        " {\"report_id\": int, \n",
        "  \"dup_id\": int or None (the report id which it is duplicate), \n",
        "  \"component\": string, \n",
        "  \"product\": string, \n",
        "  \"summary\": str, \n",
        "  \"description\": string, \n",
        "  \"creation_date\": int} \n",
        "```\n",
        "\n",
        "For instance, when extract_data_from_page receives \"bug_report/7526.html\", it returns:\n",
        "\n",
        "```\n",
        "{\n",
        "\"report_id\": 7526,\n",
        "\"dup_id\": 7799,\n",
        "\"product\": \"core graveyard\",\n",
        "\"component\":  tracking,\n",
        "\"summary\": \"Apprunner crashes on exit\",\n",
        "\"description\": \"Apprunner crashes on exit, without fail. The browser window closes, but the\n",
        "console window hangs around. I end up having to kill winoldap with the \\\\\"Close\n",
        "Program\\\\\" dialog (Ctrl-Alt-Del).\",\n",
        "\"creation_date\": 928396140\n",
        "}\n",
        "```\n",
        "\n",
        "**Creation date have to be represented as timestamp. If bug report is not duplicate, dup_id have to be None.**\n",
        "\n",
        "*HINT: lxml parse is faster than html.parser*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgE2xuiF9ZsC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKv3_jACFt0_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b474a70e-542f-4cb6-b12c-91568e44d90a"
      },
      "source": [
        "!pip install bs4\n",
        "from bs4 import BeautifulSoup\n",
        "!pip install lxml\n",
        "import re\n",
        "# define the Dictionary report\n",
        "def extract_data_from_page(pagepath):\n",
        "    report={\n",
        "        \"report_id\": int,\n",
        "        \"dup_id\": int or None,\n",
        "        \"component\": str,\n",
        "        \"product\": str,\n",
        "        \"summary\": str,\n",
        "        \"description\": str,\n",
        "        \"creation_date\": int\n",
        "    }\n",
        "    with open(pagepath,\"r\",  encoding=\"UTF-8\") as fp:\n",
        "        soup = BeautifulSoup(fp, \"lxml\")\n",
        "    \n",
        "    report[\"report_id\"]=int(re.search(r\"[0-9]+\",soup.find(\"span\", {\"id\":\"field-value-bug_id\"}).getText()).group())\n",
        "\n",
        "    report[\"creation_date\"]=int(soup.find(\"span\",{\"class\":\"rel-time\", \"data-time\":True})[\"data-time\"])\n",
        "\n",
        "    report[\"summary\"]=soup.find(\"h1\", {\"id\":\"field-value-short_desc\"}).getText()\n",
        "\n",
        "\n",
        "    report[\"component\"] = \"\".join(soup.find(\"span\", {\"id\": \"component-name\"}).getText().splitlines())\n",
        "\n",
        "\n",
        "    report[\"product\"] = \"\".join(soup.find(\"span\", {\"id\": \"product-name\"}).getText().splitlines())\n",
        "\n",
        "    dup_id=soup.find(\"span\",{\"id\":\"field-value-status-view\"})\n",
        "    if dup_id.find('a'):\n",
        "      report[\"dup_id\"]=int(re.search(r\"[0-9]+\",dup_id.find('a').getText()).group())\n",
        "    else:\n",
        "      report[\"dup_id\"]=None\n",
        "    \n",
        "    report['description']=soup.find(\"pre\", {\"class\":\"comment-text\"}).getText()\n",
        "    return(report)\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (4.2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU7f5OIzFt1E"
      },
      "source": [
        "## 4.3 - Extraire le texte du code HTML / Extract text from HTML\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9w8BxGjgFt1E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "414819ac-ab8c-49f3-89fb-e93f7db40ae8"
      },
      "source": [
        "import os\n",
        "from multiprocessing import Pool, TimeoutError\n",
        "from time import time\n",
        "import json\n",
        "import tqdm\n",
        "\n",
        "# Index each thread by its id\n",
        "index_path = os.path.join(PAGE_FOLDER, 'bug_reports.json')\n",
        "\n",
        "if os.path.isfile(index_path):\n",
        "    # Load threads that webpage content were already extracted.\n",
        "    report_index = json.load(open(index_path))\n",
        "else:\n",
        "    # Extract webpage content\n",
        "\n",
        "    # This can be slow (around 10 minutes). Test your code with a small sample. lxml parse is faster than html.parser\n",
        "    files = [os.path.join(PAGE_FOLDER, filename) for filename in os.listdir(PAGE_FOLDER)]\n",
        "    reports = [extract_data_from_page(f) for f in tqdm.tqdm(files)]\n",
        "    report_index = dict(((report['report_id'], report) for report in reports ))\n",
        "\n",
        "    # Save preprocessed bug reports\n",
        "    json.dump(report_index, open(index_path,'w'))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9998/9998 [09:25<00:00, 17.69it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBlKamszJrzn"
      },
      "source": [
        "index_path = os.path.join(PAGE_FOLDER, 'bug_reports.json')\n",
        "report_index = json.load(open(index_path))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pd5Sx79Ft1G"
      },
      "source": [
        "# 5 - Prétraitement des données / Data Preprocessing\n",
        "\n",
        "Le prétraitement des données est une tache cruciale en fouille de données. Cette étape nettoie et transforme les données brutes dans un format qui permet leur analyse, et leur utilisation avec des algorithmes de *machine learning*. En traitement des langages (natural language processing, NLP), la *tokenization* et le *stemming* sont des étapes cruciales. De plus, vous implémenterez une étape supplémentaire pour filtrer les mots sans importance.\n",
        "\n",
        "---\n",
        "\n",
        "Preprocessing is a crucial task in data mining. The objective is to clean and transform the raw data in a format that is better suited for data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well-known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
        "\n",
        "## 5.1 - Tokenization\n",
        "\n",
        "Cette étape permet de séparer un texte en séquence de *tokens* (= jetons, ici des mots, symboles ou ponctuation). Par example, la phrase *\"It's the student's notebook.\"* peut être séparé en liste de tokens de cette manière: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n",
        "\n",
        "---\n",
        "\n",
        "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuation). For instance, the sentence *\"It's the student's notebook.\"* can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
        "\n",
        "\n",
        "### 5.1.1 - Question 2 (0.5 points) \n",
        "Implémentez la fonction suivante :\n",
        "\n",
        "- **tokenize_space** qui tokenize le texte à partir des blancs (espace, tabulation, nouvelle ligne). Ce tokenizer est naïf.\n",
        "- **tokenize_nltk** qui utilise le tokenizer par défaut de la librairie nltk (https://www.nltk.org/api/nltk.html).\n",
        "- **tokenize_space_punk** replace la ponctuation par des espaces puis tokenize les tokens qui sont séparés par des blancs (espace, tabulation, retour à la ligne).\n",
        "\n",
        "**Tous les tokenizers doivent mettre les tokens en minuscule.**\n",
        "\n",
        "---\n",
        "\n",
        "Implement the following functions: \n",
        "- **tokenize_space** tokenizes the tokens that are separated by whitespace (space, tab, newline). This is a naive tokenizer.\n",
        "- **tokenize_nltk** uses the default method of the nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
        "- **tokenize_space_punk** replaces the punctuation to space and then tokenizes the tokens that are separated by whitespace (space, tab, newline).\n",
        "\n",
        "**All tokenizers have to lowercase the tokens.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zM6Yq1pFt1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc39e89b-0455-4300-e1ba-3c1be845f330"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize_space(text):\n",
        "    return (text.lower().split())\n",
        "\n",
        "\n",
        "def tokenize_nltk(text):\n",
        "    return (word_tokenize(text.lower()))\n",
        "\n",
        "\n",
        "def tokenize_space_punk(text):\n",
        "    return (re.split(r'[\\s\\t\\n]+',re.sub(r'\\W',' ',text.lower())))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpFMlhFTFt1J"
      },
      "source": [
        "## 5.2 - Suppression des mots vides / Stop words removal\n",
        "\n",
        "### 5.2.1 - Question 3 (0.5 point)\n",
        "\n",
        "Certains tokens sont sans importance pour la comparaison, car ils apparaissent dans la majorité des discussions. Les supprimer réduit la dimension du vecteur et accélère les calculs.\n",
        "\n",
        "Expliquez quels tokens sont sans importances pour la comparaison des discussions. Implémentez la fonction filter_tokens qui retire ces mots de la liste des tokens.\n",
        "\n",
        "---\n",
        "\n",
        "There are a set of tokens that are not significant to the similarity comparison since they appear in most of bug report pages. Thus, removing them decreases the vector dimensionality and turns the similarity calculation computationally cheaper.\n",
        "\n",
        "Describe the tokens that can be removed without affecting the similarity comparison? Moreover, implement the function filter_tokens that removes these words from a list of tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tSrWT4wlLOS"
      },
      "source": [
        "# **The words that can be removed from a text that will not affect the similarity comparison are called stop words. The updated list of stop words are a list of 179 words which mainly include pronouns, auxilliary verbs, possessive adjectives and pronouns, preposition, conjunctions, adverbs, modal verbs and their negation. these kinds of words do not add new information to the text. Thereofore, in similarity comparison they do not have any value. In addition, since these words are very common and repititive, removing these words will significantly decrease  both computation time and storage volume. **\n",
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usdvFGFHFt1J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "15cd5811-2f9b-4aaa-ab6c-089c5068d801"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "def filter_tokens(tokens):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    return([token for token in tokens if token not in stop_words])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AITFRrPRFt1L"
      },
      "source": [
        "## 5.3 - Racinisation / Stemming\n",
        "\n",
        "La racinisation, ou *stemming* en anglais, est un procédé de transformation des flexions en leur radical ou racine. Par exemple, en anglais, la racinisation de \"fishing\", \"fished\" and \"fish\" donne \"fish\" (stem). \n",
        "\n",
        "---\n",
        "\n",
        "The process to convert tokens with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*. For instance, the word \"fishing\", \"fished\" , \"fish\" and \"fisher\" are reduced to the stem \"fish\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAY10IY_Ft1L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "afde36f6-a6e8-4d70-eb14-57fb008c54f0"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "\n",
        "word1 = ['I', 'tried', 'different', 'fishes']\n",
        "\n",
        "print([ stemmer.stem(w) for w in word1])\n",
        "\n",
        "word2 = ['I', 'will', 'tries', 'only', 'one', 'fishing']\n",
        "print([ stemmer.stem(w) for w in word2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'tri', 'differ', 'fish']\n",
            "['i', 'will', 'tri', 'onli', 'one', 'fish']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnm9AJPPFt1M"
      },
      "source": [
        "### 5.3.1 - Question 4 (0.5 point) \n",
        "\n",
        "Expliquez comment et pourquoi le stemming est utile à notre système.\n",
        "\n",
        "---\n",
        "\n",
        "Explain how stemming can benefit our system?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgnJBXzxFt1O"
      },
      "source": [
        "\n",
        "# **The stemming will reduce the words to their stem and map several derivative of a word to the original stem. This step in pre-processing decrease the number of words in the text and create common entries between texts which will lead to higher similarity between texts.**\n",
        "**bold text**# New Section\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqwI0Y3OFt1O"
      },
      "source": [
        "# 6 - Représentation des données / Data Representation\n",
        "\n",
        "# 6.1 - Bag of Words\n",
        "\n",
        "De nombreux algorithmes demandent des entrées qui soient toutes de la même taille, ce qui n'est forcément le cas pour des types de données comme les textes, qui peuvent avoir un nombre variable de mots.  \n",
        "\n",
        "Par exemple, considérons la phrase 1, ”Board games are much better than video games” et la phrase 2, ”Monopoly is an awesome game!”. La table ci-dessous montre un exemple d'un moyen de représentation de ces deux phrases en utilisant une représentation fixe : \n",
        "\n",
        "|<i></i>     | an | are | ! | Monopoly | awesome | better | games | than | video | much | board | is | game |\n",
        "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
        "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
        "| Sentence 2 | 1  | 0   | 1 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
        "\n",
        "Chaque colonne représente un mot du vocabulaire (de longueur 13), tandis que chaque ligne contient l'occurrence des mots dans une phrase. Ainsi, la valeur 2 à la position (1,7) est due au fait que le mot *\"games\"* apparaît deux fois dans la phrase 1. \n",
        "\n",
        "Ainsi, chaque ligne étant de longueur 13, on peut les utiliser comme vecteur pour représenter les phrases 1 et 2. Ainsi, c'est cette méthode que l'on appelle *Bag-of-Words* : c'est une représentation de documents par des vecteurs dont la dimension est égale à la taille du vocabulaire, et qui est construite en comptant le nombre d'occurrences de chaque mot. Ainsi, chaque token est ici associé à une dimension.\n",
        "\n",
        "---\n",
        "\n",
        "Many algorithms only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two tweets: ”Board games are much better than video games” and ”Monopoly is an awesome game!”. These sentences are respectively named as Sentences 1 and 2. The table below depicts how we could represent both sentences using a fixed representation.\n",
        "\n",
        "|            | an | are | ! | monopoly | awesome | better | games | than | video | much | board | is | game |\n",
        "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
        "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
        "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
        "\n",
        "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
        "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
        "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
        "could use those vectors to represent the sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e. an integer.\n",
        "\n",
        "<!-- Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
        "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
        "weight of these words will be much bigger than the other ones which can give them more\n",
        "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
        "those with high frequency) appears in most of the documents and, therefore, they do not\n",
        "help to discriminate documents. For instance, the word *of* appears in a significant\n",
        "number of tweets. Thus, having the word *of* does not make\n",
        "documents more or less similar. However, the word *terrible* is rarer and documents that\n",
        "have this word are more likely to be negative. TF-IDF is a technique that overcomes the word frequency disadvantages. -->\n",
        "\n",
        "### 6.1.2 - Question 5 (2 points)\n",
        "\n",
        "\n",
        "Implémentez le Bag-of-Words en pondérant le vecteur par la fréquence de chaque mot.\n",
        "\n",
        "**Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe [sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html) de scipy.**\n",
        "\n",
        "---\n",
        "\n",
        "Implement the bag-of-words model that weights the vector with the absolute word frequency.\n",
        "\n",
        "**For this exercise, you cannot use any external python library (e.g. scikit-learn). However, if you have a problem with memory size, you can use the scipy class [sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html).**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl7ZwKPqFt1P"
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "def transform_count_bow(X):\n",
        "    data=[]\n",
        "    indptr = [0]\n",
        "    indices = []\n",
        "    bag_words = {}\n",
        "    i=0\n",
        "    for sentence in X:\n",
        "        for token in sentence:\n",
        "            if token not in bag_words:\n",
        "                bag_words[token]=i\n",
        "                i+=1\n",
        "    for sentence in X:\n",
        "        tf={}\n",
        "        for token in sentence:\n",
        "            if token not in tf:\n",
        "                tf[token]=1\n",
        "            elif token in tf:\n",
        "                tf[token]+=1\n",
        "        \n",
        "        for word in tf:\n",
        "            data.append(tf[word])\n",
        "            index = bag_words[word]\n",
        "            indices.append(index)\n",
        "        indptr.append(len(indices))\n",
        "    sentence_matrix=csr_matrix((data, indices, indptr), dtype=int)\n",
        "    \n",
        "            \n",
        "   \n",
        "    return (sentence_matrix)\n",
        "\n",
        "                   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbp7EwyYFt1R"
      },
      "source": [
        "## 6.2 - TF-IDF\n",
        "\n",
        "L'utilisation de la fréquence d'apparition brute des mots, comme c'est le cas avec le bag-of-words, peut être problématique. En effet, peu de tokens auront une fréquence très élevée dans un document, et de ce fait, le poids de ces mots sera beaucoup plus grand que les autres, ce qui aura tendance à biaiser l'ensemble des poids. De plus, les mots qui apparaissent dans la plupart des documents n'aident pas à les discriminer. Par exemple, le mot \"*de*\" apparaît dans beaucoup de documents de la base de données, et pour autant, avoir ce mot en commun ne permet pas de conclure que des documents sont similaires. Au contraire, le mot \"*génial*\" est plus rare, mais les documents qui contiennent ce mot sont plus susceptibles d'être positifs. TF-IDF est donc une méthode qui permet de pallier à ce problème.\n",
        "\n",
        "TF-IDF pondère le vecteur en utilisant une fréquence de document inverse (IDF) et une fréquence de termes (TF).\n",
        "\n",
        "TF est l'information locale sur l'importance qu'a un mot dans un document donné, tandis que IDF mesure la capacité de discrimination des mots dans un jeu de données. \n",
        "\n",
        "L'IDF d'un mot se calcule de la façon suivante:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\text{idf}_i = \\log\\left( \\frac{N}{\\text{df}_i} \\right),\n",
        "\\end{equation}\n",
        "\n",
        "Avec $N$ le nombre de documents dans la base de données, et $\\text{df}_i$ le nombre de documents qui contiennent le mot $i$.\n",
        "\n",
        "Le nouveau poids $w_{ij}$ d'un mot $i$ dans un document $j$ peut ensuite être calculé de la façon suivante:\n",
        "\n",
        "\\begin{equation}\n",
        "  w_{ij} = \\text{tf}_{ij} \\times \\text{idf}_i,\n",
        "\\end{equation}\n",
        "\n",
        "avec $\\text{tf}_{ij}$ la fréquence du mot $i$ dans le document $j$\n",
        "\n",
        "---\n",
        "\n",
        "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
        "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
        "weight of these words will be much bigger than the other ones which can give them more\n",
        "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
        "those with high frequency) appears in most of the documents and, therefore, they do not\n",
        "help to discriminate documents. For instance, the word *of* appears in a significant\n",
        "number of tweets. Thus, having the word *of* does not make\n",
        "documents more or less similar. However, the word *aeroplane* is rarer and documents that\n",
        "have this word are more likely to be similar. TF-IDF is a technique that overcomes the word frequency disadvantages.\n",
        "\n",
        "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
        "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors. \n",
        "The following equation computes the word IDF:\n",
        "\\begin{equation}\n",
        "  idf_i = \\log\\left( \\frac{N}{df_i} \\right),\n",
        "\\end{equation}\n",
        "Where $N$ is the number of documents in the dataset $df_i$ is the number of documents that contain a word $i$.\n",
        "The new weight $w_{ij}$ of a word $i$ in a document $j$ using TF-IDF is computed as:\n",
        "\\begin{equation}\n",
        "  w_{ij} = tf_{ij} \\times idf_i,\n",
        "\\end{equation}\n",
        "Where $tf_{ij}$ is the term frequency of words $i$ in the document $j$.\n",
        "\n",
        "\n",
        "\n",
        "### 6.2.1 - Question 6 (2.5 points)\n",
        "\n",
        "Implémentez le bag-of-words avec la pondération de TF-IDF\n",
        "\n",
        "**Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe [sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html) de scipy.**\n",
        "\n",
        "---\n",
        "\n",
        "Implement a bag-of-words model that weights the vector using TF-IDF.\n",
        "\n",
        "**For this exercise, you cannot use any external python library (e.g. scikit-learn). However, if you have a problem with memory size, you can use the scipy class [sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fh0vq12Ft1S"
      },
      "source": [
        "import math\n",
        "from scipy.sparse import csr_matrix\n",
        "def transform_tf_idf_bow(X):\n",
        "    bag_words={}\n",
        "    IDF={}  \n",
        "    i=0\n",
        "    t={}\n",
        "    indptr = [0]\n",
        "    indices = []\n",
        "    data = []\n",
        "    \n",
        "    for sentence in X:\n",
        "        word_list=[]\n",
        "        for token in sentence:\n",
        "            if token not in word_list:\n",
        "                word_list.append(token)\n",
        "                if token not in bag_words:\n",
        "                    bag_words[token]=i\n",
        "                    i+=1\n",
        "                    t[token]=1\n",
        "                    \n",
        "                elif token in bag_words:\n",
        "                    t[token]+=1\n",
        "                IDF[token]=math.log((len(X)/t[token]))\n",
        "    \n",
        "    for sentence in X:\n",
        "        tf={}\n",
        "        for token in sentence:\n",
        "            if token not in tf:\n",
        "                tf[token]=1\n",
        "            elif token in tf:\n",
        "                tf[token]+=1\n",
        "        \n",
        "        for word in tf:\n",
        "            data.append(tf[word]*IDF[word])\n",
        "            index = bag_words[word]\n",
        "            indices.append(index)\n",
        "        indptr.append(len(indices))\n",
        "    sentence_matrix=csr_matrix((data, indices, indptr))      \n",
        "\n",
        "    return (sentence_matrix)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTPmhqzlFt1U"
      },
      "source": [
        "# 7 - Notre système / Our System\n",
        "\n",
        "## 7.1 - Question 7 (1 point)\n",
        "\n",
        "La *pipeline* est la séquence d'étapes de prétraitement des données qui transforme les données brutes dans un format qui permet leur analyse. Pour notre problème, implémentez un pipeline composé des étapes suivantes :\n",
        "\n",
        "1. Concatène le texte du résumé et description\n",
        "2. Tokenize le texte, retire les stop words et stem le tokens. \n",
        "3. Génère la représentation vectorielle avec transform_tf_idf_bow ou transform_count_bow.\n",
        "4. Encode données catégorielles (produit et composant) en entier \n",
        "\n",
        "La pipeline (fonction nlp_pipeline) prend en entrée la liste des rapports de bogues (liste des dictionnaires qui contiennent les informations des rapports), le type de tokenization, le type de vectorizer, un booléen qui active ou désactive la suppression des tokens inutiles et un booléen qui active ou désactive le stemming. La fonction nlp_pipeline retourne un tuple ($p$, $c$, $M$) :\n",
        "- $p$ est le vecteur qui contient l'identifiant des produits des rapports de bogues\n",
        "- $c$ est le vecteur qui contient l'identifiant des composants des rapports de bogues\n",
        "- $M$ est une matrice avec la représentation du texte.\n",
        "\n",
        "Le i-ème élément de $p$, $c$ et $M$ sont le produit, composant et la représentation du texte du i-ème rapport de bogue.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The pipeline is a sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. For our problem, you have to implement a pipeline that:\n",
        "\n",
        "1. Concatenate the summary and description\n",
        "2. Perform the tokenization, stop word removal and stemming in textual data\n",
        "3. Generate the vector representation using transform_tf_idf_bow or transform_count_bow\n",
        "4. Encode the categorical data (the component and product) to integers\n",
        "\n",
        "\n",
        "The pipeline (nlp_pipeline function) receives a list of bug reports (dictionary that contains the report information), tokenization type, vectorizer type, a flag that enables or disable the insignificant token removal and a flag that turn stemming on or off. The nlp_pipeline function returns a tuple ($p$, $c$, $M$):\n",
        "- $p$ is a vector that contains the product values of the bug reports\n",
        "- $c$ is a vector that contains the component values of the bug reports\n",
        "- $M$ is a matrix with the text representation.\n",
        "\n",
        "The $i$-th element of $p$, $c$ and $M$ are the product, component and text representation of the $i$-th report in bug_reports, respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeI39WT6Ft1U"
      },
      "source": [
        "def nlp_pipeline(bug_reports, tokenization_type, vectorizer_type, enable_stop_words, enable_stemming):\n",
        "  M=[]\n",
        "  c=[]\n",
        "  p=[]\n",
        "  component_dict={}\n",
        "  product_dict={}\n",
        "  i=0\n",
        "  j=0\n",
        "  tokens_list=[]\n",
        "  bug_reports_list=[]\n",
        "  for bug_report in bug_reports:\n",
        "    bug_reports_list.append(\" \".join([bug_report['summary'],bug_report['description']]))\n",
        "    if bug_report['component'] not in component_dict:\n",
        "      component_dict[bug_report['component']]=i\n",
        "      c.append(i)\n",
        "      i+=1\n",
        "    elif bug_report['component'] in component_dict:\n",
        "      c.append(component_dict[bug_report['component']])\n",
        "    if bug_report['product'] not in product_dict:\n",
        "      product_dict[bug_report['product']]=j\n",
        "      p.append(j)\n",
        "      j+=1\n",
        "    elif bug_report['product']  in product_dict:\n",
        "      p.append(product_dict[bug_report['product']])\n",
        "  for report in bug_reports_list:\n",
        "    if tokenization_type is \"space_tokenization\":\n",
        "      tokens=tokenize_space(report) \n",
        "    elif tokenization_type is \"nltk_tokenization\":\n",
        "      tokens=tokenize_nltk(report)\n",
        "    elif tokenization_type is \"space_punk_tokenization\":\n",
        "      tokens=tokenize_space_punk(report)\n",
        "    if enable_stop_words is True:\n",
        "      tokens=filter_tokens(tokens)\n",
        "   \n",
        "    if enable_stemming is True:\n",
        "      tokens=[stemmer.stem(token) for token in tokens]\n",
        "    \n",
        "    tokens_list.append(tokens)\n",
        "  \n",
        "  if vectorizer_type is \"count\":\n",
        "    M=transform_count_bow(tokens_list)\n",
        "  elif vectorizer_type is \"tf_idf\":\n",
        "    M=transform_tf_idf_bow(tokens_list)\n",
        "  \n",
        "  return(p,c,M)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BHMKa_yFt1W"
      },
      "source": [
        "## 7.2 - Question 8 (1 point)\n",
        "\n",
        "Implémentez la fonction rank qui retourne la liste des indices des rapports de bogues triée en fonction de la similarité entre les rapports de bug (candidat) et du nouveau rapport (requête). Vous utiliserez la fonction de similarité suivante pour comparer deux rapports :\n",
        "\n",
        "$$\n",
        "\\mathrm{SIM}(q,r) = w_1 * F_1(q,r) + w_2 * F_c(q,r) + w_3 * cos\\_sim(\\mathrm{txt}_q, \\mathrm{txt}_c),\n",
        "$$\n",
        "$$\n",
        " F_p(q,r) = \\begin{cases}\n",
        "    1 ,& \\text{si } p_q= p_r\\\\\n",
        "    0,              & \\text{autrement},\n",
        "\\end{cases}\n",
        "$$\n",
        "$$\n",
        " F_p(q,r) = \\begin{cases}\n",
        "    1 ,& \\text{si } c_q = c_r\\\\\n",
        "    0,              & \\text{autrement},\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Où $c_q$ et $c_r$ sont les composants de la requête et du candidat,\n",
        " $p_q$ et $p_r$ sont les produits de la requête et du candidat,\n",
        " $\\mathrm{txt}_q$ et $\\mathrm{txt}_c$ sont les représentations du texte de la requête et du candida. Les paramètres \n",
        " w_1, w_2 et w_3 doivent être réglés.\n",
        " \n",
        "\n",
        "**Pour cette question, la requête doit être retirée de la liste triée (sortie de la fonction rank).**\n",
        "\n",
        "*Pour de meilleures performances, vous pouvez utiliser la fonction cos d'une libraire (par exemple [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)) pour calculer la similarité. Il est préférable de faire des opérations matricielles.*\n",
        "\n",
        "---\n",
        "\n",
        "Implement the function rank that returns a list of reports indexes sorted by similarity of the bug reports (candidates) and new bug report (query). We use the following similarity function to compare two bug reports:\n",
        "\n",
        "$$\n",
        "\\mathrm{SIM}(q,r) = w_1 * F_1(q,r) + w_2 * F_c(q,r) + w_3 * cos\\_sim(\\mathrm{txt}_q, \\mathrm{txt}_c),\n",
        "$$\n",
        "$$\n",
        " F_p(q,r) = \\begin{cases}\n",
        "    1 ,& \\text{if } p_q= p_r\\\\\n",
        "    0,              & \\text{otherwise},\n",
        "\\end{cases}\n",
        "$$\n",
        "$$\n",
        " F_p(q,r) = \\begin{cases}\n",
        "    1 ,& \\text{if } c_q = c_r\\\\\n",
        "    0,              & \\text{otherwise},\n",
        "\\end{cases}\n",
        "$$\n",
        "Where $c_q$ and $c_r$ are the query and candidate components,\n",
        " $p_q$ and $p_r$ are the query and candidate products,\n",
        " $\\mathrm{txt}_q$ and $\\mathrm{txt}_c$ are the query and candidate textual representations, respectively. The parameters \n",
        " w_1, w_2 and w_3 are to be tuned.\n",
        " \n",
        "\n",
        "**In this question, the query has to  be removed in the sorted list (rank output).**\n",
        "\n",
        "*For better performance, you can use the cosine similarity from a library (e.g. [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)). Also, we recommend performing matrix operations.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f37FYL8Ft1W"
      },
      "source": [
        "from sklearn.metrics.pairwise  import cosine_similarity\n",
        "import numpy as np\n",
        "def rank(query_idx, p, c, M, w1, w2, w3):\n",
        "  p_ar=np.array(p)\n",
        "  c_ar=np.array(c)\n",
        "  \n",
        "  cos_sim=np.transpose(cosine_similarity(M,M[query_idx:query_idx+1]))\n",
        "  FP=np.where(p_ar==p_ar[query_idx],1,0)\n",
        "  FC=np.where(c_ar==c_ar[query_idx],1,0)\n",
        "     \n",
        "  sim=np.argsort(w1*FP+w2*FC+w3*cos_sim)\n",
        "  return (sim[sim!=query_idx][::-1])\n",
        " \n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lSM6p8AFt1Y"
      },
      "source": [
        "## 7.3 - Évaluation / Evaluation\n",
        "\n",
        "Vous allez tester différentes configurations du système de recommandations. Ces configurations seront comparées avec la [mean average precision (MAP) metric](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision). Plus les discussions pertinentes sont recommandées rapidement (c.-à-d. en haut de la liste), plus élevé sera le score MAP. Ressources supplémentaires pour comprendre MAP: [recall and precision over ranks](https://youtu.be/H7oAofuZjjE) et [MAP](https://youtu.be/pM6DJ0ZZee0).\n",
        "\n",
        "\n",
        "La fonction *eval* évalue une configuration spécifique du système\n",
        "\n",
        "---\n",
        "\n",
        "We will test different configurations of our recommender system. These configurations are compared using the [mean average precision (MAP) metric](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision). Basically, the closer relevant threads are from ranked list begining, the higher MAP is. Additional materials to undertand MAP: [recall and precision over ranks](https://youtu.be/H7oAofuZjjE) and [MAP](https://youtu.be/pM6DJ0ZZee0).\n",
        "\n",
        "\n",
        "The function *eval* evaluates a specific configurantion of our system\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyz5u-yCFt1Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "939ebd26-935e-4568-e029-10220ca8ef08"
      },
      "source": [
        "from statistics import mean \n",
        "\n",
        "\n",
        "def calculate_map(x):\n",
        "    res = 0.0\n",
        "    n = 0.0\n",
        "    \n",
        "    for query_id, corrects, candidate_ids in x:\n",
        "        precisions = []\n",
        "        for k, candidate_id in enumerate(candidate_ids):\n",
        "            \n",
        "            if candidate_id in corrects:\n",
        "                prec_at_k = (len(precisions) + 1)/(k+1)\n",
        "                precisions.append(prec_at_k)\n",
        "                \n",
        "            if len(precisions) == len(corrects):\n",
        "                break\n",
        "                            \n",
        "        res += mean(precisions)\n",
        "        n += 1\n",
        "    \n",
        "    return res/n\n",
        "            \n",
        "\n",
        "def eval(tokenization_type, vectorizer, enable_stop_words, enable_stemming, w1, w2, w3):\n",
        "    reports = [r for r in report_index.values()]\n",
        "    report_ids = [r[\"report_id\"] for r in report_index.values()]\n",
        "    prod_v, comp_v, M = nlp_pipeline(reports, tokenization_type, vectorizer, enable_stop_words, enable_stemming)\n",
        "    report2idx = dict([(r['report_id'], idx) for idx,r in enumerate(reports)])\n",
        "    rank_lists = []\n",
        "    for query_id, corrects in test:\n",
        "        query_idx =  report_ids.index(query_id)\n",
        "        candidate_idxs = rank(query_idx, prod_v, comp_v, M, w1, w2, w3)\n",
        "        candidate_ids = [ report_ids[idx] for idx in candidate_idxs]\n",
        "                \n",
        "        rank_lists.append((query_id, corrects, candidate_ids))\n",
        "        \n",
        "        \n",
        "    return calculate_map(rank_lists)\n",
        "\n",
        "#eval( \"space_tokenization\", \"count\", False, False, 1, 1, 7)   \n",
        "#eval( \"nltk_tokenization\", \"count\", False, False, 1, 1, 7)\n",
        "#eval( \"space_punk_tokenization\", \"count\", False, False, 0.7, 0.7, 5)\n",
        "#eval( \"space_punk_tokenization\", \"count\", True, False, .7, .7, 8)\n",
        "#eval( \"space_punk_tokenization\", \"count\", True, True, 0.8, 0.8, 10)\n",
        "#eval( \"space_punk_tokenization\", \"tf_idf\", False, False, 5, 6, 100)\n",
        "#eval( \"space_punk_tokenization\", \"tf_idf\", True, False, 7, 7, 152)\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, True, 7, 7, 152)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2208218424059815"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMaHKg_WFt1a"
      },
      "source": [
        "## 7.4 - Question 9 (4 points)\n",
        "\n",
        "Évaluez votre système avec chacune des configurations suivantes :\n",
        "\n",
        "1. count(BoW) + space_tokenization\n",
        "2. count(BoW) + nltk_tokenization\n",
        "2. count(BoW) + space_punk_tokenization\n",
        "3. count(BoW) + space_punk_tokenization + Stop words removal\n",
        "4. count(BoW) + space_punk_tokenization + Stop words removal + Stemming\n",
        "5. tf_idf + space_punk_tokenization\n",
        "6. tf_idf + space_punk_tokenization + Stop words removal\n",
        "7. tf_idf + space_punk_tokenization + Stop words removal + Stemming \n",
        "\n",
        "**Pour chaque configuration :** \n",
        "\n",
        "1. Rapportez la performance avec $w_1=0$ et $w_2=0$\n",
        "\n",
        "2. Réglez les paramètres $w_1$, $w_2$ et $w_3$ et rapportez les valeurs qui donne les 3 meilleurs et les 3 pires résultats.\n",
        "\n",
        "**En plus, décrivez et comparez vos résultats. Répondez aux questions suivantes :**\n",
        "\n",
        "- Quelle méthode de tokenization donne les meilleurs résultats ? À votre avis, pourquoi ?\n",
        "- Les étapes de prétraitement ont-elles un impact positif ou négatif sur notre système ?\n",
        "- Est-ce que TF-IDF permet d'obtenir de meilleures performances que CountBoW? Si oui, à votre avis, pourquoi ?\n",
        "- Est-ce que la comparaison du composant et du produit affecte positivement notre méthode ? \n",
        "\n",
        "**Notez qu'il y a une valeur minimum de  MAP à atteindre pour chaque configuration en dessous de laquelle la question sera pénalisée de 50%.**\n",
        "\n",
        "1. count(BoW) + space_tokenization: 0.090 \n",
        "2. count(BoW) + nltk_tokenization: 0.090\n",
        "2. count(BoW) + space_punk_tokenization: 0.120\n",
        "3. count(BoW) + space_punk_tokenization + Stop words removal: 0.170\n",
        "4. count(BoW) + space_punk_tokenization + Stop words removal + Stemming: 0.195\n",
        "5. tf_idf + space_punk_tokenization: 0.210\n",
        "6. tf_idf + space_punk_tokenization + Stop words removal: 0.210\n",
        "7. tf_idf + space_punk_tokenization + Stop words removal + Stemming: 0.215\n",
        "\n",
        "---\n",
        "\n",
        "Evaluate the system using each one of the following configurations:\n",
        "\n",
        "1. count(BoW) + space_tokenization\n",
        "2. count(BoW) + nltk_tokenization\n",
        "2. count(BoW) + space_punk_tokenization\n",
        "3. count(BoW) + space_punk_tokenization + Stop words removal\n",
        "4. count(BoW) + space_punk_tokenization + Stop words removal + Stemming\n",
        "5. tf_idf + space_punk_tokenization\n",
        "6. tf_idf + space_punk_tokenization + Stop words removal\n",
        "7. tf_idf + space_punk_tokenization + Stop words removal + Stemming \n",
        "\n",
        "**For each configuration:** \n",
        "\n",
        "1. Report the method performance achieved when $w_1=0$ and $w_2=0$\n",
        "\n",
        "2. Tune the parameters $w_1$, $w_2$ and $w_3$ and report the parameter values that achieve the 3 best and 3 worst results.\n",
        "\n",
        "**Also, describe and compare the results found by you and answer the following questions:**\n",
        "\n",
        "- Which tokenization strategy has achieved the best result? Why do you think this has occurred?\n",
        "- Was our system negatively or positively impacted by data preprocessing steps?\n",
        "- TF-IDF has achieved a better performance than CountBoW? If yes, why do you think that this has occurred?\n",
        "- Did the component and product comparison positively affect our method? \n",
        "\n",
        "**Note that there is a minimum MAP value to achieve for each configuration (see below) below which the question will be penalized by 50%.**\n",
        "\n",
        "1. count(BoW) + space_tokenization: 0.090 \n",
        "2. count(BoW) + nltk_tokenization: 0.090\n",
        "2. count(BoW) + space_punk_tokenization: 0.120\n",
        "3. count(BoW) + space_punk_tokenization + Stop words removal: 0.170\n",
        "4. count(BoW) + space_punk_tokenization + Stop words removal + Stemming: 0.195\n",
        "5. tf_idf + space_punk_tokenization: 0.210\n",
        "6. tf_idf + space_punk_tokenization + Stop words removal: 0.210\n",
        "7. tf_idf + space_punk_tokenization + Stop words removal + Stemming: 0.215"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMUv5OkP6Q2C"
      },
      "source": [
        " *Which tokenization strategy has achieved the best result? Why do you think this has occurred?*\n",
        "**space_punk_tokenization\" gives the best similarity score.  Because it removes punctuations, links, etc,  therefore the similarity of texts increases. In \"space_tokenization\" pontuations adhered to the words that causes to have different words for a single word. for instance, \"(size, size., 'size, \"size\", size\" are all the same but \"space_tokenization\" produce different tokens for that single word. The \"nltk_tokenization\" consider punctuations and symbols (like @,#) as a token and create false similarity while they add no information for similarity. On the other hand, \"space_punk_tokenization\" removes punctuations, paths, links and etc. therefore keeps tokens that add information to the text. In addition, \"space_punk_tokenization\" produces less tokens that need less computation and storage resources.** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6gHxt4WTpG7"
      },
      "source": [
        "*Was our system negatively or positively impacted by data preprocessing steps?* \n",
        "**Preprocessing steps are crucial and necessary for every analysis. The preprocessing methods removes unnecessary texts and transform textual data into numerical variable that is suitable for machine learning algorithms. These steps are crucial for similarity comparisons. Data of about 10,000 HTML files are extracted in less than 10 minutes and information is analyzed and sorted in about 1 minute which makes the future analysis very easy.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRWF56vnVm_V"
      },
      "source": [
        "\n",
        "*TF-IDF has achieved a better performance than CountBoW? If yes, why do you think that this has occurred?* \n",
        "**Yes TF-IDF has better performance than \"Count\" approach. While \"count\" only returns the frequency of a word in a text, \"TF_IDF\" measure relevance of a word in whole dataset.In other words this approach measure importance of words in a dataset (in constrast to \"count\" that measure frequency of words in the sentence. This is very useful to find similarity between sentences in a dataset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV2AdR8fZYKK"
      },
      "source": [
        "*Did the component and product comparison positively affect our method?* **Yes, incorporating product and component improve MAP value which shows comparison of these two entries improve ouput of recommendation system. unlike \"summary\" and \"description\" section, these two sections have limited entries (\"component\" and \"product\" should be selected from drop-down list by customer), therefore similarity between tokens are higher that makes these two sections very effective in similarity comparison. This is evidenty can be seen when comparing MAP value when w1=w2=0 with other cases that these weights are non-zero.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKr4vHDMLZrY"
      },
      "source": [
        "```\n",
        "eval( \"space_tokenization\", \"count\", False, False, 0, 0, 1)=0.0838803596072975   \n",
        "eval( \"nltk_tokenization\", \"count\", False, False, 0, 0, 1)=0.0818958631605452\n",
        "eval( \"space_punk_tokenization\", \"count\", False, False, 0, 0, 1)=0.11539725767824005\n",
        "eval( \"space_punk_tokenization\", \"count\", True, False, 0, 0, 1)=0.16943314296473047\n",
        "eval( \"space_punk_tokenization\", \"count\", True, True, 0, 0, 1)=0.19874981707562836\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", False, False, 0, 0, 1)=0.20327563980859167\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, False, 0, 0, 1)=0.20169687363315852\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, True, 0, 0, 1)=0.21078433195606228 \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1-Dgtojzfny"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "eval( \"space_tokenization\", \"count\", False, False, 0.14, 0.13, 6) =0.09018535103352153\n",
        "eval( \"space_tokenization\", \"count\", False, False, 0.13, 0.14, 6)=0.09022904411303871\n",
        "eval( \"space_tokenization\", \"count\", False, False, 0.12, 0.14, 6)=0.09007635984918488\n",
        "\n",
        "eval( \"space_tokenization\", \"count\", False, False, 1, 1, 7)=0.09802229146823004\n",
        "eval( \"space_tokenization\", \"count\", False, False, 1, 1, 8)=0.09854910909309975\n",
        "eval( \"space_tokenization\", \"count\", False, False, 1, 1, 9)=0.09828799823854747\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-24vNIe7QaJ"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "eval( \"nltk_tokenization\", \"count\", False, False, 0.18, 0.19, 6)= 0.09083751108718754\n",
        "eval( \"nltk_tokenization\", \"count\", False, False, 0.17, .19, 6)=0.0908727356136755\n",
        "eval( \"nltk_tokenization\", \"count\", False, False, 0.175, .19, 6)=0.09080823508139146\n",
        "\n",
        "eval( \"nltk_tokenization\", \"count\", False, False, 1, 1, 7)=0.09748633527356529\n",
        "eval( \"nltk_tokenization\", \"count\", False, False, 1, 1, 5)=0.09800387831990245\n",
        "eval( \"nltk_tokenization\", \"count\", False, False, 1, 1, 6)=0.09784149321571817\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKir6q9EU8qP"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "eval( \"space_punk_tokenization\", \"count\", False, False, 0.04, 0.07, 6)=0.120846055858826\n",
        "eval( \"space_punk_tokenization\", \"count\", False, False, .05, .07, 6)=0.12119123673526033\n",
        "eval( \"space_punk_tokenization\", \"count\", False, False, .05, .07, 6.3)=0.12090685535010048\n",
        "\n",
        "eval( \"space_punk_tokenization\", \"count\", False, False, .6, .6, 6)=0.1328096951481551\n",
        "eval( \"space_punk_tokenization\", \"count\", False, False, .7, .7, 9)=0.13255759713493448\n",
        "eval( \"space_punk_tokenization\", \"count\", False, False, .7, .7, 5)=0.13351961590452624\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b_2yZ1Tebei"
      },
      "source": [
        "```\n",
        "eval( \"space_punk_tokenization\", \"count\", True, False, .01, .01, 6)=0.1702661375318209\n",
        "eval( \"space_punk_tokenization\", \"count\", True, False, .01, .01, 5)=0.1702944397093868\n",
        "eval( \"space_punk_tokenization\", \"count\", True, False, .1, .1, 4)=0.17046565891488336\n",
        "\n",
        "eval( \"space_punk_tokenization\", \"count\", True, False, .8, .8, 9)=0.1811179981526148\n",
        "eval( \"space_punk_tokenization\", \"count\", True, False, .7, .7, 9)=0.18210629850721272\n",
        "eval( \"space_punk_tokenization\", \"count\", True, False, .7, .7, 8)=0.18117506829007374\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w-6ExQUo60V"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "eval( \"space_punk_tokenization\", \"count\", True, True, .03, .01, 0.08)=0.19587898970966247\n",
        "eval( \"space_punk_tokenization\", \"count\", True, True, .04, .01, 0.08)=0.19504341183513155\n",
        "eval( \"space_punk_tokenization\", \"count\", True, True, .02, .02, .135)=0.19511153916057217\n",
        "\n",
        "eval( \"space_punk_tokenization\", \"count\", True, True, .8, .8, 9)=0.20523882798447549\n",
        "eval( \"space_punk_tokenization\", \"count\", True, True, .7, .7, 9)=0.20505891595199038\n",
        "eval( \"space_punk_tokenization\", \"count\", True, True, .8, .8, 10)=0.20518648691530994\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQEU6NlccgZ_"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", False, False, .2, .2, 9.5)=0.21022354440060112\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", False, False, .2, .2, 9.6)=0.21011504763075045\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", False, False, .2, .2, 9.7)=0.21004802842077883\n",
        "\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", False, False, 5, 6, 100)=0.21593175562706007\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", False, False, 6, 6, 100)=0.21638593996660763\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", False, False, 6, 7, 100)=0.2163596784645163\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-brwsZeZ7ujD"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, False, .2, .2, 9.5)=0.21043315544810065\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, False, .2, .2, 9.6, 10)=0.21043845214744733\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, False, .2, .2, 9.7)=0.21077359372563237\n",
        "\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, False, 7, 7, 150)=0.2161148820585441\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, False, 7, 7, 151)=0.216142135433857\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, False, 7, 7, 152)=0.21610963593512728\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sRzSpQEcJ3a"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, True, .1, .1, 8.9)=0.21527317884944822\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, True, .1, .1, 9)=0.2152623072477104\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, True, .1, .1, 8.8)=0.21531085568786323\n",
        "\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, True, 7, 7, 150)= 0.22094843112784524\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, True, 7, 7, 151)=0.22078638262359943\n",
        "eval( \"space_punk_tokenization\", \"tf_idf\", True, True, 7, 7, 152)=0.2208218424059815\n",
        "```\n",
        "\n"
      ]
    }
  ]
}